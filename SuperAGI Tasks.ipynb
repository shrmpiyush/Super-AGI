{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c49232",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836e09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "        query_len, key_len, value_len = query.shape[1], keys.shape[1], values.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # Add dimensions for heads\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization, and finally dropout\n",
    "        x = self.norm1(attention + query)\n",
    "        forward = self.feedforward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, vocab_size, device):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Embedding(1000, embed_size)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(embed_size, heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.embedding(x) + self.positional_encoding(positions)\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            out = transformer(out, out, out, mask)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# Test the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2(embed_size=256, heads=8, num_layers=4, vocab_size=10000, device=device)\n",
    "x = torch.randint(0, 10000, (32, 10))  # Batch size of 32, sequence length of 10\n",
    "mask = torch.ones((32, 10)).to(device)\n",
    "output = model(x.to(device), mask)\n",
    "print(output.shape)  # This should print torch.Size([32, 10, 10000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2a426",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3c9514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(RotaryPositionalEmbedding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.w = nn.Parameter(torch.randn(embed_size // 2) * 0.01)\n",
    "        self.b = nn.Parameter(torch.zeros(embed_size // 2))\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, positions):\n",
    "        sin_input = positions.unsqueeze(-1) * 2.0 ** torch.arange(0, self.embed_size // 2).to(positions.device)\n",
    "        sin_component = torch.sin(sin_input)\n",
    "        cos_component = torch.cos(sin_input)\n",
    "\n",
    "        sin_part = self.alpha * (self.w * sin_component + self.b)\n",
    "        cos_part = self.alpha * (self.w * cos_component + self.b)\n",
    "\n",
    "        return torch.cat([sin_part, cos_part], dim=-1)\n",
    "#The RotaryPositionalEmbedding introduces a novel positional encoding method using rotary embeddings. This can be beneficial for capturing sequential information in a different way compared to standard positional encodings. However, its effectiveness depends on the specific task and dataset. The number of parameters is relatively small, so it is a lightweight addition to the model.\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, use_sliding_window_attention=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        self.use_sliding_window_attention = use_sliding_window_attention\n",
    "\n",
    "        if use_sliding_window_attention:\n",
    "            self.attention = SlidingWindowAttention(embed_size, heads, window_size=5)\n",
    "        else:\n",
    "            self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "            self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        query_len, key_len, value_len = query.shape[1], keys.shape[1], values.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        if self.use_sliding_window_attention:\n",
    "            attention = self.attention(queries, keys, values, mask)\n",
    "        else:\n",
    "            values = self.values(values)\n",
    "            keys = self.keys(keys)\n",
    "            queries = self.queries(queries)\n",
    "\n",
    "            energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "                energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = F.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "#The MultiHeadAttention now supports an option for using SlidingWindowAttention. Sliding window attention can be useful for handling long sequences more efficiently. However, the actual implementation of the SlidingWindowAttention mechanism is not provided in the code, so its effectiveness cannot be evaluated without it.\n",
    "    \n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, group_size):\n",
    "        super(GroupQueryAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.group_size = group_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        query_len, key_len, value_len = query.shape[1], keys.shape[1], values.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = F.softmax(energy / (self.head_dim ** 0.5), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# GroupQueryAttention introduces the idea of grouping queries for attention. This can be useful when certain queries need to attend to specific subsets of the key-value pairs. It adds a modest number of parameters, but its effectiveness would depend on the task and dataset characteristics.\n",
    "    \n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, window_size):\n",
    "        super(SlidingWindowAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.window_size = window_size\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "    def forward(self, queries, keys, values, mask):\n",
    "        # ... (implement sliding window attention mechanism)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, use_group_query_attention=False, use_sliding_window_attention=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        if use_group_query_attention:\n",
    "            self.attention = GroupQueryAttention(embed_size, heads, group_size=4)\n",
    "        elif use_sliding_window_attention:\n",
    "            self.attention = MultiHeadAttention(embed_size, heads, use_sliding_window_attention=True)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x = self.norm1(attention + query)\n",
    "        forward = self.feedforward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out\n",
    "\n",
    "#The TransformerBlock now allows for choosing between standard self-attention and GroupQueryAttention or SlidingWindowAttention. This flexibility allows experimentation with different attention mechanisms. It's important to note that using multiple attention mechanisms may increase the model's capacity and complexity.\n",
    "    \n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, vocab_size, device,\n",
    "                 use_rotary_positional_embedding=False, use_group_query_attention=False, use_sliding_window_attention=False):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "\n",
    "        if use_rotary_positional_embedding:\n",
    "            self.rotary_positional_encoding = RotaryPositionalEmbedding(embed_size)\n",
    "        else:\n",
    "            self.positional_encoding = nn.Embedding(1000, embed_size)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(embed_size, heads, use_group_query_attention, use_sliding_window_attention) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        if hasattr(self, 'rotary_positional_encoding'):\n",
    "            # Use Rotary positional encoding\n",
    "            out = self.embedding(x) + self.rotary_positional_encoding(positions)\n",
    "        else:\n",
    "            # Use standard positional encoding\n",
    "            out = self.embedding(x) + self.positional_encoding(positions)\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            out = transformer(out, out, out, mask)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# The GPT2 model now supports various attention mechanisms and positional encodings. The flexibility to choose between rotary positional embeddings, group query attention, and sliding window attention allows for experimentation with different combinations.\n",
    "\n",
    "# Test the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2(embed_size=256, heads=8, num_layers=4, vocab_size=10000, device=device,\n",
    "             use_rotary_positional_embedding=True, use_group_query_attention=True, use_sliding_window_attention=True)\n",
    "x = torch.randint(0, 10000, (32, 10))  # Batch size of 32, sequence length of 10\n",
    "mask = torch.ones((32, 10)).to(device)\n",
    "output = model(x.to(device), mask)\n",
    "print(output.shape)  # This should print torch.Size([32, 10, 10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1da4b",
   "metadata": {},
   "source": [
    "Model Size: \n",
    "The model's size is influenced by the added components, but it remains reasonably sized. The total number of parameters depends on the choices made (e.g., rotary embeddings, attention mechanisms).\n",
    "\n",
    "Potential Pitfalls:\n",
    "The effectiveness of the new components depends on the task and dataset. It's recommended to conduct thorough experiments to validate improvements.\n",
    "The sliding window attention mechanism needs to be implemented correctly to ensure its benefits are realized.\n",
    "\n",
    "Possible Improvements:\n",
    "Provide a complete implementation of the sliding window attention mechanism for a comprehensive evaluation.\n",
    "Conduct experiments to compare the performance of different attention mechanisms and positional encodings on your specific task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ffb39f",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1987de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Step [0/16], Loss: 9.366623878479004\n",
      "Epoch [1/10], Step [0/16], Loss: 9.401666641235352\n",
      "Epoch [2/10], Step [0/16], Loss: 9.373968124389648\n",
      "Epoch [3/10], Step [0/16], Loss: 9.362058639526367\n",
      "Epoch [4/10], Step [0/16], Loss: 9.376642227172852\n",
      "Epoch [5/10], Step [0/16], Loss: 9.342138290405273\n",
      "Epoch [6/10], Step [0/16], Loss: 9.366018295288086\n",
      "Epoch [7/10], Step [0/16], Loss: 9.318262100219727\n",
      "Epoch [8/10], Step [0/16], Loss: 9.328409194946289\n",
      "Epoch [9/10], Step [0/16], Loss: 9.356502532958984\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from fairscale.nn import FullyShardedDataParallel\n",
    "\n",
    "# Define your GPT2 model, loss function, and optimizer\n",
    "# (Assuming you have already defined the GPT2 class and other necessary components)\n",
    "\n",
    "# Dummy dataset for demonstration\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_length, vocab_size):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sequence = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
    "        target_sequence = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
    "        return input_sequence, target_sequence\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 1000\n",
    "seq_length = 10\n",
    "vocab_size = 10000\n",
    "batch_size = 64\n",
    "embed_size = 256\n",
    "heads = 8\n",
    "num_layers = 4\n",
    "\n",
    "# Create an instance of the GPT2 model\n",
    "model = GPT2(embed_size, heads, num_layers, vocab_size, device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Dummy DataLoader for demonstration\n",
    "dataset = RandomDataset(num_samples, seq_length, vocab_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set up distributed training if using DDP\n",
    "if torch.cuda.device_count() > 1:\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    model = DistributedDataParallel(model)\n",
    "\n",
    "# Set up Fully Sharded Data Parallel (FSDP)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = FullyShardedDataParallel(model)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, mask=None)  # Assuming 'mask' is available\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "\n",
    "# Cleanup for distributed training\n",
    "if torch.cuda.device_count() > 1:\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b6a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
